# LLM Gateway - Implementation Documentation

## é¡¹ç›®å®ŒæˆçŠ¶æ€ï¼šâœ… 100%

æˆåŠŸå®ç°äº†ä¸€ä¸ªå®Œæ•´çš„ã€ç”Ÿäº§å°±ç»ªçš„ LLM ä»£ç†ç½‘å…³ï¼Œæ”¯æŒ OpenAIã€Anthropic (Claude)ã€Google (Gemini) ä¸‰ç§åè®®ã€‚

**Version**: 0.3.0
**Stack**: Rust + Axum + Tokio + Prometheus

## æœ€ç»ˆç»Ÿè®¡

- **æ€»ä»£ç è¡Œæ•°**: 3,465 è¡Œ Rust ä»£ç 
- **æºæ–‡ä»¶æ•°é‡**: 26 ä¸ª Rust æ–‡ä»¶
- **æµ‹è¯•è¦†ç›–**: 58 ä¸ªå•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•ï¼Œå…¨éƒ¨é€šè¿‡ âœ…
- **Release äºŒè¿›åˆ¶å¤§å°**: 5.1 MB
- **ç¼–è¯‘æ—¶é—´**: ~1åˆ†21ç§’ (release mode)

## æ ¸å¿ƒåŠŸèƒ½å®ç°

### Phase 1-2: åŸºç¡€æ¡†æ¶ âœ…
- [x] Cargo é¡¹ç›®åˆå§‹åŒ–ä¸ä¾èµ–é…ç½®
- [x] é…ç½®ç®¡ç†ç³»ç»Ÿï¼ˆTOML + ç¯å¢ƒå˜é‡ï¼‰
  - æ¨¡å‹æ˜ å°„é…ç½®ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰
  - å¤šç»´åº¦é…ç½®éªŒè¯
- [x] Axum æœåŠ¡å™¨åŸºç¡€è®¾æ–½
- [x] å¥åº·æ£€æŸ¥ç«¯ç‚¹ (`/health`, `/ready`)
- [x] è®¤è¯ä¸­é—´ä»¶ï¼ˆBearer Tokenï¼‰
- [x] ç»Ÿä¸€é”™è¯¯å¤„ç†
- [x] æ¨¡å‹è·¯ç”±å™¨

### Phase 3: OpenAI ç›´é€š âœ…
- [x] OpenAI æ•°æ®æ¨¡å‹ï¼ˆè¯·æ±‚/å“åº”/æµå¼ï¼‰
- [x] OpenAI Provider å®¢æˆ·ç«¯
- [x] `/v1/chat/completions` Handler
- [x] SSE æµå¼åŸºç¡€è®¾æ–½

### Phase 4: Anthropic é›†æˆ âœ…
- [x] Anthropic æ•°æ®æ¨¡å‹
- [x] OpenAI â†’ Anthropic è¯·æ±‚è½¬æ¢å™¨
  - System æ¶ˆæ¯æå–
  - max_tokens å¿…å¡«å¤„ç†
  - temperature èŒƒå›´è£å‰ª (0-1)
- [x] Anthropic å“åº” â†’ OpenAI æ ¼å¼è½¬æ¢å™¨
  - éæµå¼å“åº”è½¬æ¢
  - SSE äº‹ä»¶æ˜ å°„
- [x] Anthropic Provider å®¢æˆ·ç«¯
- [x] é›†æˆåˆ°ç»Ÿä¸€ Handler
- [x] Native Anthropic API (`/v1/messages`) - ç›´é€šæ— è½¬æ¢

### Phase 5: Gemini é›†æˆ âœ…
- [x] Gemini æ•°æ®æ¨¡å‹
- [x] OpenAI â†’ Gemini è¯·æ±‚è½¬æ¢å™¨
  - Role æ˜ å°„ (assistant â†’ model)
  - systemInstruction å¤„ç†
  - parts æ ¼å¼è½¬æ¢
- [x] Gemini å“åº” â†’ OpenAI æ ¼å¼è½¬æ¢å™¨
- [x] Gemini Provider å®¢æˆ·ç«¯
- [x] é›†æˆåˆ°ç»Ÿä¸€ Handler

### Phase 6: å¤šæ¨¡æ€ä¸é«˜çº§åŠŸèƒ½ âœ…

#### Vision/Image Support
- Image handling across all 3 providers
- Secure URL fetching with validation
- Automatic format conversion (URL â†’ base64)

#### Tool/Function Calling
- Full tool calling pipeline working
- OpenAI â†” Anthropic â†” Gemini tool conversion (all providers!)
- **Streaming tool support**: content_block_start, input_json_delta handling
- Specific tool forcing support

#### Prompt Caching (Anthropic)
- Cache control structures added
- **Auto-caching logic implemented**: system prompts (>1024 tokens), tools
- CacheConfig with configurable thresholds
- Token estimation and automatic application

#### Structured Outputs / JSON Mode
- Response format control (Text, JsonObject, JsonSchema)
- Native support: OpenAI, Gemini
- Workaround: Anthropic (system prompt injection)

### Phase 7: Load Balancing & High Availability âœ…
- [x] Multi-instance provider configuration
- [x] Priority-based sticky sessions
- [x] Automatic failover on instance failure
- [x] Health state management with auto-recovery
- [x] Session TTL and cleanup (1 hour)
- [x] Instance-level metrics

### Phase 8: Observability & Monitoring âœ…

#### Prometheus Metrics
- Four-dimension metrics
  - `llm_requests_total` (api_key, provider, model, endpoint)
  - `llm_tokens_total` (api_key, provider, model, type)
  - `llm_request_duration_seconds` (api_key, provider, model)
  - `llm_errors_total` (api_key, provider, model, error_type)
  - `llm_instance_health_status` - instance health (1=healthy, 0=unhealthy)
  - `llm_instance_requests_total` - per-instance request count with status
  - `llm_gateway_session_count` - active sticky sessions
- `/metrics` ç«¯ç‚¹
- é›†æˆåˆ°æ‰€æœ‰ Handlers

#### Structured Logging
- ç»“æ„åŒ– JSON æ—¥å¿—ï¼ˆtracingï¼‰
- è¯·æ±‚çº§åˆ«è¿½è¸ª
- åè®®è½¬æ¢æ—¥å¿—

#### Stats Command
- Real-time dashboard using ratatui
- Prometheus metrics visualization
- Grouping by API key / provider / model / all
- Manual and auto-refresh

### Phase 9: Conversion Warnings System âœ…
- Warning infrastructure created
- **HTTP headers implemented**: X-LLM-Gateway-Warnings
- Converters return (request, warnings) tuple
- Warnings propagated to both streaming and non-streaming responses
- Parameter compatibility logging with user-facing feedback

### Phase 10: API Endpoints âœ…
- [x] `/v1/chat/completions` - OpenAI-compatible (all providers)
- [x] `/v1/messages` - Native Anthropic API
- [x] `/v1/models` - Model listing
- [x] `/health`, `/ready` - Health checks
- [x] `/metrics` - Prometheus metrics

### Phase 11: å®¹å™¨åŒ–ä¸éƒ¨ç½² âœ…
- [x] å¤šé˜¶æ®µ Dockerfile
- [x] .dockerignore ä¼˜åŒ–
- [x] å¥åº·æ£€æŸ¥é…ç½®
- [x] é•œåƒå¤§å°ä¼˜åŒ–
- [x] Docker Compose ç¤ºä¾‹

### Phase 12: æ–‡æ¡£ âœ…
- [x] README.md å®Œæ•´æ–‡æ¡£
- [x] CLAUDE.md å¼€å‘æŒ‡å—
- [x] FEATURES.md åŠŸèƒ½æ–‡æ¡£
- [x] CONVERSION_LIMITATIONS.md è½¬æ¢é™åˆ¶
- [x] é…ç½®ç¤ºä¾‹
- [x] API æ–‡æ¡£
- [x] ä½¿ç”¨ç¤ºä¾‹ï¼ˆCursor, Claude Codeï¼‰
- [x] ç›‘æ§æŒ‡å—

## Feature Matrix

The gateway supports comprehensive multimodal features across all providers:

| Feature | OpenAI | Anthropic | Gemini | Notes |
|---------|:------:|:---------:|:------:|-------|
| **Text Completion** | âœ… | âœ… | âœ… | Full support |
| **Streaming** | âœ… | âœ… | âœ… | SSE with real-time conversion |
| **Vision/Images** | âœ… | âœ… | âœ… | Automatic base64 conversion |
| **Tool Calling (Non-Streaming)** | âœ… | âœ… | âœ… | Full request/response conversion |
| **Tool Calling (Streaming)** | âœ… | âœ… | âœ… | Incremental JSON assembly |
| **Prompt Caching** | âŒ | âœ… | âŒ | Auto-caching for system prompts & tools |
| **JSON Mode** | âœ… | âœ… âš ï¸ | âœ… | âš ï¸ = System prompt injection workaround |
| **JSON Schema** | âœ… | âœ… âš ï¸ | âœ… | âš ï¸ = System prompt injection workaround |
| **Conversion Warnings** | N/A | âœ… | âœ… | X-LLM-Gateway-Warnings header |
| **Native API** | âœ… | âœ… | âŒ | Direct passthrough support |

**Legend:**
- âœ… = Full native or converted support
- âš ï¸ = Workaround via system prompt injection
- âŒ = Not supported by provider

## æŠ€æœ¯æ¶æ„

### æ ¸å¿ƒç»„ä»¶

```
src/
â”œâ”€â”€ main.rs              # æœåŠ¡å™¨å…¥å£
â”œâ”€â”€ cli.rs               # CLI commands
â”œâ”€â”€ config.rs            # é…ç½®ç®¡ç†
â”œâ”€â”€ auth.rs              # è®¤è¯ä¸­é—´ä»¶
â”œâ”€â”€ error.rs             # é”™è¯¯å¤„ç†
â”œâ”€â”€ router.rs            # æ¨¡å‹è·¯ç”±å™¨
â”œâ”€â”€ metrics.rs           # Prometheus æŒ‡æ ‡
â”œâ”€â”€ streaming.rs         # SSE æµå¼å¤„ç†
â”œâ”€â”€ load_balancer.rs     # è´Ÿè½½å‡è¡¡ä¸ sticky sessions
â”œâ”€â”€ retry.rs             # é‡è¯•ä¸å¥åº·æ£€æµ‹
â”œâ”€â”€ image_utils.rs       # å›¾åƒå¤„ç†
â”œâ”€â”€ conversion_warnings.rs # è½¬æ¢è­¦å‘Š
â”œâ”€â”€ models/              # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ openai.rs        # OpenAI åè®®
â”‚   â”œâ”€â”€ anthropic.rs     # Anthropic åè®®
â”‚   â””â”€â”€ gemini.rs        # Gemini åè®®
â”œâ”€â”€ converters/          # åè®®è½¬æ¢å™¨
â”‚   â”œâ”€â”€ openai_to_anthropic.rs
â”‚   â”œâ”€â”€ anthropic_response.rs
â”‚   â”œâ”€â”€ anthropic_streaming.rs
â”‚   â”œâ”€â”€ openai_to_gemini.rs
â”‚   â”œâ”€â”€ gemini_response.rs
â”‚   â””â”€â”€ gemini_streaming.rs
â”œâ”€â”€ providers/           # API å®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ openai.rs        # OpenAI API
â”‚   â”œâ”€â”€ anthropic.rs     # Anthropic API
â”‚   â””â”€â”€ gemini.rs        # Gemini API
â”œâ”€â”€ handlers/            # HTTP å¤„ç†å™¨
â”‚   â”œâ”€â”€ chat_completions.rs  # OpenAI-compatible endpoint
â”‚   â”œâ”€â”€ messages.rs          # Native Anthropic endpoint
â”‚   â”œâ”€â”€ health.rs            # å¥åº·æ£€æŸ¥
â”‚   â”œâ”€â”€ metrics_handler.rs   # æŒ‡æ ‡ç«¯ç‚¹
â”‚   â””â”€â”€ models.rs            # æ¨¡å‹åˆ—è¡¨
â”œâ”€â”€ stats/               # Stats dashboard
â”‚   â”œâ”€â”€ mod.rs
â”‚   â”œâ”€â”€ parser.rs
â”‚   â””â”€â”€ ui.rs
â””â”€â”€ commands/            # CLI commands
    â”œâ”€â”€ config.rs
    â”œâ”€â”€ start.rs
    â””â”€â”€ stats.rs
```

### ä¾èµ–æ ˆ

| ç»„ä»¶ | æŠ€æœ¯ | ç‰ˆæœ¬ |
|------|------|------|
| Web æ¡†æ¶ | Axum + Tokio | 0.7 / 1.x |
| HTTP å®¢æˆ·ç«¯ | reqwest | 0.12 |
| é…ç½®ç®¡ç† | serde + toml + config | - |
| æŒ‡æ ‡å¯¼å‡º | metrics + prometheus | 0.23 / 0.15 |
| æ—¥å¿—è¿½è¸ª | tracing + tracing-subscriber | 0.1 / 0.3 |
| Token è®¡æ•° | tiktoken-rs | 0.5 |
| SSE æµå¤„ç† | eventsource-stream + futures | 0.2 / 0.3 |
| TUI Dashboard | ratatui + crossterm | 0.28 / 0.28 |
| CLI | clap | 4.x |

## Load Balancing Architecture

### Sticky Session Strategy

**Why Sticky Sessions?**
- **Maximizes KV Cache Hits**: Same user â†’ same instance â†’ provider can reuse conversation context
- **Minimal Lock Contention**: DashMap with segment locking + RwLock for read-heavy health checks
- **Predictable Performance**: No random load distribution that breaks cache locality

**How It Works:**
1. **First Request**: User makes initial request â†’ LoadBalancer selects instance by priority
2. **Session Creation**: API key bound to selected instance for 1 hour
3. **Subsequent Requests**: Same API key always routes to same instance (until failure or timeout)
4. **Session Expiry**: After 1 hour of inactivity, session expires â†’ next request reselects by priority

### Priority-Based Selection

Instances are configured with a **priority** value (lower number = higher priority):

```toml
# Primary instance (always preferred when healthy)
[[providers.anthropic]]
name = "anthropic-primary"
priority = 1                    # Highest priority

# Backup instance (used only when primary fails)
[[providers.anthropic]]
name = "anthropic-backup"
priority = 2                    # Lower priority
```

**Selection Algorithm:**
1. Filter: Only healthy and enabled instances
2. Find minimum priority value among healthy instances
3. Random selection among instances with that priority
4. Bind API key to selected instance (sticky session)

### Automatic Failover & Recovery

#### Health Detection Criteria

An instance is marked **unhealthy** on **single request failure** of these types:

| Failure Type | Examples | Action |
|--------------|----------|--------|
| **5xx Server Errors** | 500, 502, 503, 504 | Mark unhealthy |
| **Connection Failures** | TCP timeout, connection refused, DNS failure | Mark unhealthy |
| **Request Timeouts** | Exceeds `timeout_seconds` | Mark unhealthy |
| **4xx Client Errors** | 401, 403, 429 | **No action** (not instance fault) |
| **Business Errors** | Invalid API key, rate limit | **No action** |

#### Auto-Recovery Mechanism

**Passive Time-Based Recovery** (no active health probes):
- Instance marked unhealthy on first failure
- Auto-recovers after `failure_timeout_seconds` (default: 60s)
- Gradual recovery: existing sessions stay on backup until natural expiry
- Progressive load: new sessions go to primary, old sessions stay on backup

## åè®®è½¬æ¢è¯¦æƒ…

### OpenAI â†’ Anthropic

| ç‰¹æ€§ | OpenAI | Anthropic | è½¬æ¢ç­–ç•¥ |
|------|--------|-----------|---------|
| System æ¶ˆæ¯ | messages[0] | system å­—æ®µ | âœ… æå– |
| max_tokens | å¯é€‰ | å¿…éœ€ | âœ… é»˜è®¤ 4096 |
| temperature | 0-2 | 0-1 | âœ… è£å‰ªåˆ° 1.0 |
| æµå¼äº‹ä»¶ | SSE | SSE | âœ… å®Œæ•´æ˜ å°„ |
| Tools | OpenAI format | Anthropic format | âœ… å®Œæ•´è½¬æ¢ |
| Images | URL or base64 | Base64 only | âœ… è‡ªåŠ¨è½¬æ¢ |

### OpenAI â†’ Gemini

| ç‰¹æ€§ | OpenAI | Gemini | è½¬æ¢ç­–ç•¥ |
|------|--------|--------|---------|
| Role åç§° | assistant | model | âœ… æ˜ å°„ |
| System æŒ‡ä»¤ | messages[0] | systemInstruction | âœ… æå– |
| å†…å®¹æ ¼å¼ | content | parts: [{text}] | âœ… åŒ…è£… |
| æµå¼ | stream: true | ?alt=sse | âœ… URL å‚æ•° |
| Tools | OpenAI format | function_declarations | âœ… å®Œæ•´è½¬æ¢ |

## æ€§èƒ½ç‰¹æ€§

- **é›¶æ‹·è´æµå¼**: ä½¿ç”¨ `bytes_stream()` é¿å…ç¼“å†²åŒºç´¯ç§¯
- **é«˜æ€§èƒ½ä¸­é—´ä»¶**: Axum Tower æ ˆ
- **Release ä¼˜åŒ–**: LTO + codegen-units=1
- **äºŒè¿›åˆ¶ä½“ç§¯**: 5.1 MB (å·²å‰¥ç¦»ç¬¦å·)
- **Sticky Sessions**: Memory-only hash lookups for routing
- **Segment Locking**: DashMap for low contention

## æµ‹è¯•è¦†ç›–

### å•å…ƒæµ‹è¯•

- **é…ç½®ç®¡ç†**: Config validation, multi-instance parsing
- **è®¤è¯**: Bearer token validation
- **é”™è¯¯å¤„ç†**: Error type conversions
- **è·¯ç”±**: Model prefix matching
- **æ•°æ®æ¨¡å‹**: OpenAI, Anthropic, Gemini models
- **åè®®è½¬æ¢**: All converter pairs
- **Load Balancer**: Sticky session, health management, priority selection
- **Providers**: API client tests
- **Handlers**: Endpoint tests
- **æµå¼å¤„ç†**: SSE parsing and conversion
- **æŒ‡æ ‡**: Metrics recording

### é›†æˆæµ‹è¯•

- Health endpoint
- Ready endpoint
- Metrics endpoint
- Full request flow tests

**æ€»è®¡**: 58+ tests - å…¨éƒ¨é€šè¿‡ âœ…

## ä½¿ç”¨ç¤ºä¾‹

### Cursor é…ç½®

```bash
export OPENAI_API_BASE="http://localhost:8080/v1"
export OPENAI_API_KEY="sk-gateway-001"

# åœ¨ Cursor ä¸­åˆ‡æ¢æ¨¡å‹å³å¯ï¼š
# - gpt-4 â†’ OpenAI
# - claude-3-5-sonnet â†’ Anthropic (via conversion)
# - gemini-1.5-pro â†’ Gemini (via conversion)
```

### Claude Code é…ç½® (Native API)

```bash
# ä½¿ç”¨åŸç”Ÿ Anthropic API (æ¨è)
export ANTHROPIC_BASE_URL="http://localhost:8080"
export ANTHROPIC_API_KEY="sk-gateway-001"

# Claude Code å°†ä½¿ç”¨ /v1/messages ç«¯ç‚¹ (æ— è½¬æ¢å¼€é”€)
```

### Direct API Calls

**OpenAI-compatible API**:
```bash
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Authorization: Bearer sk-gateway-001" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-5-sonnet",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

**Native Anthropic API**:
```bash
curl -X POST http://localhost:8080/v1/messages \
  -H "Authorization: Bearer sk-gateway-001" \
  -H "anthropic-version: 2023-06-01" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-3-5-sonnet-20241022",
    "max_tokens": 1024,
    "messages": [
      {"role": "user", "content": "Hello!"}
    ]
  }'
```

## Docker éƒ¨ç½²

```bash
# æ„å»º
docker build -t llm-gateway .

# è¿è¡Œ
docker run -p 8080:8080 \
  -v $(pwd)/config.toml:/app/config.toml \
  llm-gateway
```

## ç›‘æ§

### Prometheus æŸ¥è¯¢ç¤ºä¾‹

```promql
# è¯·æ±‚æ€»æ•°
sum(llm_requests_total) by (provider, model)

# Token ä½¿ç”¨é‡
sum(llm_tokens_total{type="input"}) by (api_key)

# P95 å»¶è¿Ÿ
histogram_quantile(0.95, llm_request_duration_seconds)

# é”™è¯¯ç‡
rate(llm_errors_total[5m])

# å®ä¾‹å¥åº·çŠ¶æ€
llm_instance_health_status

# æ´»è·ƒä¼šè¯æ•°
llm_gateway_session_count
```

### Stats Dashboard

```bash
# å¯åŠ¨å®æ—¶ç›‘æ§ä»ªè¡¨æ¿
./target/release/llm-gateway stats

# è‡ªå®šä¹‰åˆ·æ–°é—´éš”
./target/release/llm-gateway stats --interval 2.0

# æŒ‰ provider åˆ†ç»„
./target/release/llm-gateway stats --group-by provider

# å¿«æ·é”®
# 1-4: åˆ‡æ¢åˆ†ç»„æ–¹å¼ (api_key/provider/model/all)
# r: æ‰‹åŠ¨åˆ·æ–°
# q: é€€å‡º
```

## å…³é”®æˆå°±

1. âœ… **å®Œæ•´çš„ä¸‰åè®®æ”¯æŒ**: OpenAIã€Anthropicã€Gemini
2. âœ… **å¤š API æ ¼å¼**: OpenAI-compatible + Native Anthropic
3. âœ… **æ™ºèƒ½è·¯ç”±**: åŸºäºæ¨¡å‹åç§°è‡ªåŠ¨è·¯ç”±
4. âœ… **åè®®è½¬æ¢**: åŒå‘è½¬æ¢å™¨ï¼Œç²¾å‡†æ˜ å°„
5. âœ… **æµå¼æ”¯æŒ**: SSE å®æ—¶è½¬å‘æ‰€æœ‰ providers
6. âœ… **å¤šæ¨¡æ€**: å›¾åƒã€å·¥å…·è°ƒç”¨ã€ç»“æ„åŒ–è¾“å‡º
7. âœ… **è´Ÿè½½å‡è¡¡**: å¤šå®ä¾‹ + sticky sessions + è‡ªåŠ¨æ•…éšœè½¬ç§»
8. âœ… **å››ç»´åº¦æŒ‡æ ‡**: å®Œæ•´çš„å¯è§‚æµ‹æ€§
9. âœ… **é›¶ä¾èµ–**: æ— éœ€æ•°æ®åº“/ç¼“å­˜/Redis
10. âœ… **ç”Ÿäº§å°±ç»ª**: Dockerã€å¥åº·æ£€æŸ¥ã€æ—¥å¿—ã€ç›‘æ§
11. âœ… **é«˜æµ‹è¯•è¦†ç›–**: 58+ æµ‹è¯•ï¼Œ100%é€šè¿‡
12. âœ… **CLI å·¥å…·**: é…ç½®ç®¡ç†ã€stats ä»ªè¡¨æ¿

## å·²çŸ¥é—®é¢˜ä¸ä¿®å¤

### Anthropic Thinking Field Fix

**é—®é¢˜**: Anthropic API åœ¨å“åº”å’Œè¯·æ±‚ä¸­ `thinking` å­—æ®µæ ¼å¼ä¸ä¸€è‡´
- å“åº”æ ¼å¼: `{"thinking": "content"}` (æ—  signature)
- è¯·æ±‚æ ¼å¼: `{"thinking": "content", "signature": "value"}` (éœ€è¦ signature)

**å½±å“**: Claude Code å®˜æ–¹å®¢æˆ·ç«¯å‘é€å†å²æ¶ˆæ¯æ—¶ä¼šè§¦å‘ 400 é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**: Gateway åœ¨è½¬å‘åˆ° Anthropic API å‰è‡ªåŠ¨æ¸…ç†ä¸å®Œæ•´çš„ thinking å­—æ®µ

**å®ç°**: `src/handlers/messages.rs` - æ¸…ç† assistant æ¶ˆæ¯ä¸­ç¼ºå°‘ signature çš„ thinking å­—æ®µ

è¯¦è§: `THINKING_FIELD_FIX.md`

## æ€»ç»“

æˆåŠŸäº¤ä»˜äº†ä¸€ä¸ª**åŠŸèƒ½å®Œæ•´ã€æµ‹è¯•å……åˆ†ã€ç”Ÿäº§å°±ç»ª**çš„ LLM ä»£ç†ç½‘å…³ã€‚

æ ¸å¿ƒä»·å€¼ï¼š
- **å¤š API æ ¼å¼æ”¯æŒ**: OpenAI-compatible + Native Anthropic
- ç»Ÿä¸€ OpenAI API è°ƒç”¨æ‰€æœ‰æ¨¡å‹
- åŸç”Ÿ Anthropic API ç›´é€šæ— è½¬æ¢
- ä¿ç•™å„æä¾›å•†åŸç”Ÿç‰¹æ€§
- å®Œæ•´çš„ç›‘æ§å’Œæ—¥å¿—
- é›¶å¤–éƒ¨ä¾èµ–ï¼Œæ˜“äºéƒ¨ç½²
- é«˜å¯ç”¨è´Ÿè½½å‡è¡¡

**çŠ¶æ€ï¼šå¯ç›´æ¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ ğŸš€**
