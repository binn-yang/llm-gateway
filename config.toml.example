[server]
host = "0.0.0.0"
port = 8080
log_level = "info"
log_format = "json"

# API Keys Authentication
# Generate secure random keys for production use
[[api_keys]]
key = "sk-gateway-YOUR-KEY-HERE-001"
name = "frontend-app"
enabled = true

[[api_keys]]
key = "sk-gateway-YOUR-KEY-HERE-002"
name = "backend-service"
enabled = true

# Routing Configuration
# Model requests are routed to providers based on model name prefix matching
# The gateway is now a pass-through proxy - model names are sent directly to providers
[routing]
# Prefix matching rules (longest prefix is matched first)
# Examples:
#   "gpt-4-turbo-2024-04-09" matches "gpt-" → routes to OpenAI
#   "claude-3-5-sonnet-20241022" matches "claude-" → routes to Anthropic
#   "gemini-1.5-pro" matches "gemini-" → routes to Gemini
[routing.rules]
"gpt-" = "openai"
"o1-" = "openai"
"o3-" = "openai"
"text-embedding-" = "openai"
"claude-" = "anthropic"
"gemini-" = "gemini"
"models/gemini-" = "gemini"  # Gemini uses "models/" prefix in model names

# Default provider when no prefix matches (optional)
# If not set, requests with unknown prefixes will be rejected
default_provider = "openai"

# Model discovery and caching configuration
[routing.discovery]
enabled = true
cache_ttl_seconds = 3600      # Cache model lists for 1 hour
refresh_on_startup = true     # Fetch model lists when gateway starts
providers_with_listing = ["openai"]  # Only OpenAI has public model list API

# Provider Configurations
[providers.openai]
enabled = true
api_key = "sk-YOUR-OPENAI-API-KEY-HERE"
base_url = "https://api.openai.com/v1"
timeout_seconds = 300

[providers.anthropic]
enabled = false
api_key = "sk-ant-YOUR-ANTHROPIC-KEY-HERE"
base_url = "https://api.anthropic.com/v1"
timeout_seconds = 300
api_version = "2023-06-01"

[providers.gemini]
enabled = false
api_key = "YOUR-GEMINI-KEY-HERE"
base_url = "https://generativelanguage.googleapis.com/v1beta"
timeout_seconds = 300

# Prometheus Metrics
[metrics]
enabled = true
endpoint = "/metrics"
include_api_key_hash = true

# Observability (Logs, Traces, Metrics Persistence)
# Enable this for persistent logs, request tracing, and historical metrics
[observability]
enabled = false  # Set to true to enable SQLite-based observability
database_path = "./data/observability.db"

# Performance tuning
[observability.performance]
batch_size = 100              # Number of log entries per batch
flush_interval_ms = 100       # Max time before flushing batch (milliseconds)
max_buffer_size = 10000       # Ring buffer size to prevent blocking

# Data retention policies (automatic cleanup)
[observability.retention]
logs_days = 7                     # Keep logs for 7 days
spans_days = 7                    # Keep trace spans for 7 days
metrics_snapshots_days = 30       # Keep metrics snapshots for 30 days
cleanup_hour = 3                  # Run cleanup at 3 AM daily (0-23)

# Prometheus metrics snapshots (for historical analysis)
[observability.metrics_snapshot]
enabled = true                    # Snapshot metrics to SQLite
interval_seconds = 300            # Snapshot every 5 minutes
